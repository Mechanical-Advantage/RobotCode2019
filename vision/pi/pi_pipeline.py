#!/usr/bin/python3

"""
This program uses a generated GRIP pipeline for object tracking.

It performs calculations on the resulting geometry and publishes the results 
through ZeroMQ for use by the RoboRio.
"""
import cv2
import numpy
import math
from enum import Enum
import time
import zmq
import struct
from collections import namedtuple
import timeit
# Retro reflective tape pipeline
class GripPipelineRetro:
    """
    An OpenCV pipeline generated by GRIP.
    """
    
    def __init__(self):
        """initializes all values to presets or None if need to be set
        """

        self.__resize_image_width = 160.0
        self.__resize_image_height = 120.0
        self.__resize_image_interpolation = cv2.INTER_LINEAR

        self.resize_image_output = None

        self.__hsv_threshold_input = self.resize_image_output
        self.__hsv_threshold_hue = [57, 78]
        self.__hsv_threshold_saturation = [15, 255.0]
        self.__hsv_threshold_value = [50, 255]

        self.hsv_threshold_output = None

        self.__find_contours_input = self.hsv_threshold_output
        self.__find_contours_external_only = True

        self.find_contours_output = None

        self.__filter_contours_contours = self.find_contours_output
        self.__filter_contours_min_area = 20.0
        self.__filter_contours_min_perimeter = 30.0
        self.__filter_contours_min_width = 0.0
        self.__filter_contours_max_width = 1000.0
        self.__filter_contours_min_height = 0.0
        self.__filter_contours_max_height = 1000.0
        self.__filter_contours_solidity = [79, 100]
        self.__filter_contours_max_vertices = 1000000.0
        self.__filter_contours_min_vertices = 0.0
        self.__filter_contours_min_ratio = 0.0
        self.__filter_contours_max_ratio = 1000.0

        self.filter_contours_output = None

        self.__convex_hulls_contours = self.filter_contours_output

        self.convex_hulls_output = None


    def process(self, source0):
        """
        Runs the pipeline and sets all outputs to new values.
        """
        # Step Resize_Image0:
        #self.__resize_image_input = source0
        #(self.resize_image_output) = self.__resize_image(self.__resize_image_input, self.__resize_image_width, self.__resize_image_height, self.__resize_image_interpolation)

        # Step HSV_Threshold0:
        self.__hsv_threshold_input = source0
        (self.hsv_threshold_output) = self.__hsv_threshold(self.__hsv_threshold_input, self.__hsv_threshold_hue, self.__hsv_threshold_saturation, self.__hsv_threshold_value)

        # Step Find_Contours0:
        self.__find_contours_input = self.hsv_threshold_output
        (self.find_contours_output) = self.__find_contours(self.__find_contours_input, self.__find_contours_external_only)

        # Step Filter_Contours0:
        self.__filter_contours_contours = self.find_contours_output
        (self.filter_contours_output) = self.__filter_contours(self.__filter_contours_contours, self.__filter_contours_min_area, self.__filter_contours_min_perimeter, self.__filter_contours_min_width, self.__filter_contours_max_width, self.__filter_contours_min_height, self.__filter_contours_max_height, self.__filter_contours_solidity, self.__filter_contours_max_vertices, self.__filter_contours_min_vertices, self.__filter_contours_min_ratio, self.__filter_contours_max_ratio)

        # Step Convex_Hulls0:
        self.__convex_hulls_contours = self.filter_contours_output
        (self.convex_hulls_output) = self.__convex_hulls(self.__convex_hulls_contours)


    @staticmethod
    def __resize_image(input, width, height, interpolation):
        """Scales and image to an exact size.
        Args:
            input: A numpy.ndarray.
            Width: The desired width in pixels.
            Height: The desired height in pixels.
            interpolation: Opencv enum for the type fo interpolation.
        Returns:
            A numpy.ndarray of the new size.
        """
        return cv2.resize(input, ((int)(width), (int)(height)), 0, 0, interpolation)

    @staticmethod
    def __hsv_threshold(input, hue, sat, val):
        """Segment an image based on hue, saturation, and value ranges.
        Args:
            input: A BGR numpy.ndarray.
            hue: A list of two numbers the are the min and max hue.
            sat: A list of two numbers the are the min and max saturation.
            lum: A list of two numbers the are the min and max value.
        Returns:
            A black and white numpy.ndarray.
        """
        out = cv2.cvtColor(input, cv2.COLOR_BGR2HSV)
        return cv2.inRange(out, (hue[0], sat[0], val[0]),  (hue[1], sat[1], val[1]))

    @staticmethod
    def __find_contours(input, external_only):
        """Sets the values of pixels in a binary image to their distance to the nearest black pixel.
        Args:
            input: A numpy.ndarray.
            external_only: A boolean. If true only external contours are found.
        Return:
            A list of numpy.ndarray where each one represents a contour.
        """
        if(external_only):
            mode = cv2.RETR_EXTERNAL
        else:
            mode = cv2.RETR_LIST
        method = cv2.CHAIN_APPROX_SIMPLE
        im2, contours, hierarchy =cv2.findContours(input, mode=mode, method=method)
        return contours

    @staticmethod
    def __filter_contours(input_contours, min_area, min_perimeter, min_width, max_width,
                        min_height, max_height, solidity, max_vertex_count, min_vertex_count,
                        min_ratio, max_ratio):
        """Filters out contours that do not meet certain criteria.
        Args:
            input_contours: Contours as a list of numpy.ndarray.
            min_area: The minimum area of a contour that will be kept.
            min_perimeter: The minimum perimeter of a contour that will be kept.
            min_width: Minimum width of a contour.
            max_width: MaxWidth maximum width.
            min_height: Minimum height.
            max_height: Maximimum height.
            solidity: The minimum and maximum solidity of a contour.
            min_vertex_count: Minimum vertex Count of the contours.
            max_vertex_count: Maximum vertex Count.
            min_ratio: Minimum ratio of width to height.
            max_ratio: Maximum ratio of width to height.
        Returns:
            Contours as a list of numpy.ndarray.
        """
        output = []
        for contour in input_contours:
            x,y,w,h = cv2.boundingRect(contour)
            if (w < min_width or w > max_width):
                continue
            if (h < min_height or h > max_height):
                continue
            area = cv2.contourArea(contour)
            if (area < min_area):
                continue
            if (cv2.arcLength(contour, True) < min_perimeter):
                continue
            hull = cv2.convexHull(contour)
            solid = 100 * area / cv2.contourArea(hull)
            if (solid < solidity[0] or solid > solidity[1]):
                continue
            if (len(contour) < min_vertex_count or len(contour) > max_vertex_count):
                continue
            ratio = (float)(w) / h
            if (ratio < min_ratio or ratio > max_ratio):
                continue
            output.append(contour)
        return output

    @staticmethod
    def __convex_hulls(input_contours):
        """Computes the convex hulls of contours.
        Args:
            input_contours: A list of numpy.ndarray that each represent a contour.
        Returns:
            A list of numpy.ndarray that each represent a contour.
        """
        output = []
        for contour in input_contours:
            output.append(cv2.convexHull(contour))
        return output

class GripPipelineHatch:
    """
    An OpenCV pipeline generated by GRIP.
    """
    
    def __init__(self):
        """initializes all values to presets or None if need to be set
        """

        self.__resize_image_width = 160.0
        self.__resize_image_height = 120.0
        self.__resize_image_interpolation = cv2.INTER_CUBIC

        self.resize_image_output = None

        self.__blur_input = self.resize_image_output
        self.__blur_type = BlurType.Box_Blur
        self.__blur_radius = 1.4414422146908874

        self.blur_output = None

        self.__hsv_threshold_input = self.blur_output
        self.__hsv_threshold_hue = [22.66187050359711, 40.546076257074276]
        self.__hsv_threshold_saturation = [29.352516236065103, 156.65527184261805]
        self.__hsv_threshold_value = [150.43164789676666, 255.0]

        self.hsv_threshold_output = None

        self.__find_contours_input = self.hsv_threshold_output
        self.__find_contours_external_only = True

        self.find_contours_output = None

        self.__convex_hulls_contours = self.find_contours_output

        self.convex_hulls_output = None

        self.__filter_contours_contours = self.convex_hulls_output
        self.__filter_contours_min_area = 150.0
        self.__filter_contours_min_perimeter = 0.0
        self.__filter_contours_min_width = 0.0
        self.__filter_contours_max_width = 1000.0
        self.__filter_contours_min_height = 0.0
        self.__filter_contours_max_height = 1000.0
        self.__filter_contours_solidity = [0.0, 100.0]
        self.__filter_contours_max_vertices = 1000000.0
        self.__filter_contours_min_vertices = 12.0
        self.__filter_contours_min_ratio = 0.0
        self.__filter_contours_max_ratio = 1000.0

        self.filter_contours_output = None


    def process(self, source0):
        """
        Runs the pipeline and sets all outputs to new values.
        """
        # Step Resize_Image0:
        self.__resize_image_input = source0
        (self.resize_image_output) = self.__resize_image(self.__resize_image_input, self.__resize_image_width, self.__resize_image_height, self.__resize_image_interpolation)

        # Step Blur0:
        self.__blur_input = self.resize_image_output
        (self.blur_output) = self.__blur(self.__blur_input, self.__blur_type, self.__blur_radius)

        # Step HSV_Threshold0:
        self.__hsv_threshold_input = self.blur_output
        (self.hsv_threshold_output) = self.__hsv_threshold(self.__hsv_threshold_input, self.__hsv_threshold_hue, self.__hsv_threshold_saturation, self.__hsv_threshold_value)

        # Step Find_Contours0:
        self.__find_contours_input = self.hsv_threshold_output
        (self.find_contours_output) = self.__find_contours(self.__find_contours_input, self.__find_contours_external_only)

        # Step Convex_Hulls0:
        self.__convex_hulls_contours = self.find_contours_output
        (self.convex_hulls_output) = self.__convex_hulls(self.__convex_hulls_contours)

        # Step Filter_Contours0:
        self.__filter_contours_contours = self.convex_hulls_output
        (self.filter_contours_output) = self.__filter_contours(self.__filter_contours_contours, self.__filter_contours_min_area, self.__filter_contours_min_perimeter, self.__filter_contours_min_width, self.__filter_contours_max_width, self.__filter_contours_min_height, self.__filter_contours_max_height, self.__filter_contours_solidity, self.__filter_contours_max_vertices, self.__filter_contours_min_vertices, self.__filter_contours_min_ratio, self.__filter_contours_max_ratio)


    @staticmethod
    def __resize_image(input, width, height, interpolation):
        """Scales and image to an exact size.
        Args:
            input: A numpy.ndarray.
            Width: The desired width in pixels.
            Height: The desired height in pixels.
            interpolation: Opencv enum for the type fo interpolation.
        Returns:
            A numpy.ndarray of the new size.
        """
        return cv2.resize(input, ((int)(width), (int)(height)), 0, 0, interpolation)

    @staticmethod
    def __blur(src, type, radius):
        """Softens an image using one of several filters.
        Args:
            src: The source mat (numpy.ndarray).
            type: The blurType to perform represented as an int.
            radius: The radius for the blur as a float.
        Returns:
            A numpy.ndarray that has been blurred.
        """
        if(type is BlurType.Box_Blur):
            ksize = int(2 * round(radius) + 1)
            return cv2.blur(src, (ksize, ksize))
        elif(type is BlurType.Gaussian_Blur):
            ksize = int(6 * round(radius) + 1)
            return cv2.GaussianBlur(src, (ksize, ksize), round(radius))
        elif(type is BlurType.Median_Filter):
            ksize = int(2 * round(radius) + 1)
            return cv2.medianBlur(src, ksize)
        else:
            return cv2.bilateralFilter(src, -1, round(radius), round(radius))

    @staticmethod
    def __hsv_threshold(input, hue, sat, val):
        """Segment an image based on hue, saturation, and value ranges.
        Args:
            input: A BGR numpy.ndarray.
            hue: A list of two numbers the are the min and max hue.
            sat: A list of two numbers the are the min and max saturation.
            lum: A list of two numbers the are the min and max value.
        Returns:
            A black and white numpy.ndarray.
        """
        out = cv2.cvtColor(input, cv2.COLOR_BGR2HSV)
        return cv2.inRange(out, (hue[0], sat[0], val[0]),  (hue[1], sat[1], val[1]))

    @staticmethod
    def __find_contours(input, external_only):
        """Sets the values of pixels in a binary image to their distance to the nearest black pixel.
        Args:
            input: A numpy.ndarray.
            external_only: A boolean. If true only external contours are found.
        Return:
            A list of numpy.ndarray where each one represents a contour.
        """
        if(external_only):
            mode = cv2.RETR_EXTERNAL
        else:
            mode = cv2.RETR_LIST
        method = cv2.CHAIN_APPROX_SIMPLE
        im2, contours, hierarchy =cv2.findContours(input, mode=mode, method=method)
        return contours

    @staticmethod
    def __convex_hulls(input_contours):
        """Computes the convex hulls of contours.
        Args:
            input_contours: A list of numpy.ndarray that each represent a contour.
        Returns:
            A list of numpy.ndarray that each represent a contour.
        """
        output = []
        for contour in input_contours:
            output.append(cv2.convexHull(contour))
        return output

    @staticmethod
    def __filter_contours(input_contours, min_area, min_perimeter, min_width, max_width,
                        min_height, max_height, solidity, max_vertex_count, min_vertex_count,
                        min_ratio, max_ratio):
        """Filters out contours that do not meet certain criteria.
        Args:
            input_contours: Contours as a list of numpy.ndarray.
            min_area: The minimum area of a contour that will be kept.
            min_perimeter: The minimum perimeter of a contour that will be kept.
            min_width: Minimum width of a contour.
            max_width: MaxWidth maximum width.
            min_height: Minimum height.
            max_height: Maximimum height.
            solidity: The minimum and maximum solidity of a contour.
            min_vertex_count: Minimum vertex Count of the contours.
            max_vertex_count: Maximum vertex Count.
            min_ratio: Minimum ratio of width to height.
            max_ratio: Maximum ratio of width to height.
        Returns:
            Contours as a list of numpy.ndarray.
        """
        output = []
        for contour in input_contours:
            x,y,w,h = cv2.boundingRect(contour)
            if (w < min_width or w > max_width):
                continue
            if (h < min_height or h > max_height):
                continue
            area = cv2.contourArea(contour)
            if (area < min_area):
                continue
            if (cv2.arcLength(contour, True) < min_perimeter):
                continue
            hull = cv2.convexHull(contour)
            solid = 100 * area / cv2.contourArea(hull)
            if (solid < solidity[0] or solid > solidity[1]):
                continue
            if (len(contour) < min_vertex_count or len(contour) > max_vertex_count):
                continue
            ratio = (float)(w) / h
            if (ratio < min_ratio or ratio > max_ratio):
                continue
            output.append(contour)
        return output

class GripPipelineWhiteTape:
    """
    An OpenCV pipeline generated by GRIP to find the white tape.
    """
    
    def __init__(self):
        """initializes all values to presets or None if need to be set
        """

        self.__blur_type = BlurType.Box_Blur
        self.__blur_radius = 2

        self.blur_output = None

        self.__rgb_threshold_input = self.blur_output
        self.__rgb_threshold_red = [0.0, 255.0]
        self.__rgb_threshold_green = [146.76258992805754, 255.0]
        self.__rgb_threshold_blue = [160.52158273381295, 255.0]

        self.rgb_threshold_output = None

        self.__cv_dilate_src = self.rgb_threshold_output
        self.__cv_dilate_kernel = None
        self.__cv_dilate_anchor = (-1, -1)
        self.__cv_dilate_iterations = 1.0
        self.__cv_dilate_bordertype = cv2.BORDER_CONSTANT
        self.__cv_dilate_bordervalue = (-1)

        self.cv_dilate_output = None

        self.__find_contours_input = self.rgb_threshold_output
        self.__find_contours_external_only = False

        self.find_contours_output = None

        self.__convex_hulls_contours = self.find_contours_output

        self.convex_hulls_output = None

        self.__filter_contours_contours = self.find_contours_output
        self.__filter_contours_min_area = 0
        self.__filter_contours_min_perimeter = 0
        self.__filter_contours_min_width = 5.0
        self.__filter_contours_max_width = 80.0
        self.__filter_contours_min_height = 15.0
        self.__filter_contours_max_height = 100.0
        self.__filter_contours_solidity = [50, 100.0]
        self.__filter_contours_max_vertices = 130.0
        self.__filter_contours_min_vertices = 0
        self.__filter_contours_min_ratio = 0.0
        self.__filter_contours_max_ratio = 1.0

        self.filter_contours_output = None


    def process(self, source0):
        """
        Runs the pipeline and sets all outputs to new values.
        """
        # Step Blur0:
        self.__blur_input = source0
        (self.blur_output) = self.__blur(self.__blur_input, self.__blur_type, self.__blur_radius)

        # Step RGB_Threshold0:
        self.__rgb_threshold_input = self.blur_output
        (self.rgb_threshold_output) = self.__rgb_threshold(self.__rgb_threshold_input, self.__rgb_threshold_red, self.__rgb_threshold_green, self.__rgb_threshold_blue)

        # Step CV_dilate0:
        # self.__cv_dilate_src = self.rgb_threshold_output
        # (self.cv_dilate_output) = self.__cv_dilate(self.__cv_dilate_src, self.__cv_dilate_kernel, self.__cv_dilate_anchor, self.__cv_dilate_iterations, self.__cv_dilate_bordertype, self.__cv_dilate_bordervalue)

        # Step Find_Contours0:
        self.__find_contours_input = self.rgb_threshold_output
        (self.find_contours_output) = self.__find_contours(self.__find_contours_input, self.__find_contours_external_only)

        # Step Convex_Hulls0:
        # self.__convex_hulls_contours = self.find_contours_output
        # (self.convex_hulls_output) = self.__convex_hulls(self.__convex_hulls_contours)

        # Step Filter_Contours0:
        self.__filter_contours_contours = self.find_contours_output
        (self.filter_contours_output) = self.__filter_contours(self.__filter_contours_contours, self.__filter_contours_min_area, self.__filter_contours_min_perimeter, self.__filter_contours_min_width, self.__filter_contours_max_width, self.__filter_contours_min_height, self.__filter_contours_max_height, self.__filter_contours_solidity, self.__filter_contours_max_vertices, self.__filter_contours_min_vertices, self.__filter_contours_min_ratio, self.__filter_contours_max_ratio)


    @staticmethod
    def __blur(src, type, radius):
        """Softens an image using one of several filters.
        Args:
            src: The source mat (numpy.ndarray).
            type: The blurType to perform represented as an int.
            radius: The radius for the blur as a float.
        Returns:
            A numpy.ndarray that has been blurred.
        """
        if(type is BlurType.Box_Blur):
            ksize = int(2 * round(radius) + 1)
            return cv2.blur(src, (ksize, ksize))
        elif(type is BlurType.Gaussian_Blur):
            ksize = int(6 * round(radius) + 1)
            return cv2.GaussianBlur(src, (ksize, ksize), round(radius))
        elif(type is BlurType.Median_Filter):
            ksize = int(2 * round(radius) + 1)
            return cv2.medianBlur(src, ksize)
        else:
            return cv2.bilateralFilter(src, -1, round(radius), round(radius))

    @staticmethod
    def __rgb_threshold(input, red, green, blue):
        """Segment an image based on color ranges.
        Args:
            input: A BGR numpy.ndarray.
            red: A list of two numbers the are the min and max red.
            green: A list of two numbers the are the min and max green.
            blue: A list of two numbers the are the min and max blue.
        Returns:
            A black and white numpy.ndarray.
        """
        out = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)
        return cv2.inRange(out, (red[0], green[0], blue[0]),  (red[1], green[1], blue[1]))

    @staticmethod
    def __cv_dilate(src, kernel, anchor, iterations, border_type, border_value):
        """Expands area of higher value in an image.
        Args:
           src: A numpy.ndarray.
           kernel: The kernel for dilation. A numpy.ndarray.
           iterations: the number of times to dilate.
           border_type: Opencv enum that represents a border type.
           border_value: value to be used for a constant border.
        Returns:
            A numpy.ndarray after dilation.
        """
        return cv2.dilate(src, kernel, anchor, iterations = (int) (iterations +0.5),
                            borderType = border_type, borderValue = border_value)

    @staticmethod
    def __find_contours(input, external_only):
        """Sets the values of pixels in a binary image to their distance to the nearest black pixel.
        Args:
            input: A numpy.ndarray.
            external_only: A boolean. If true only external contours are found.
        Return:
            A list of numpy.ndarray where each one represents a contour.
        """
        if(external_only):
            mode = cv2.RETR_EXTERNAL
        else:
            mode = cv2.RETR_LIST
        method = cv2.CHAIN_APPROX_SIMPLE
        im2, contours, hierarchy =cv2.findContours(input, mode=mode, method=method)
        return contours

    @staticmethod
    def __convex_hulls(input_contours):
        """Computes the convex hulls of contours.
        Args:
            input_contours: A list of numpy.ndarray that each represent a contour.
        Returns:
            A list of numpy.ndarray that each represent a contour.
        """
        output = []
        for contour in input_contours:
            output.append(cv2.convexHull(contour))
        return output

    @staticmethod
    def __filter_contours(input_contours, min_area, min_perimeter, min_width, max_width,
                        min_height, max_height, solidity, max_vertex_count, min_vertex_count,
                        min_ratio, max_ratio):
        """Filters out contours that do not meet certain criteria.
        Args:
            input_contours: Contours as a list of numpy.ndarray.
            min_area: The minimum area of a contour that will be kept.
            min_perimeter: The minimum perimeter of a contour that will be kept.
            min_width: Minimum width of a contour.
            max_width: MaxWidth maximum width.
            min_height: Minimum height.
            max_height: Maximimum height.
            solidity: The minimum and maximum solidity of a contour.
            min_vertex_count: Minimum vertex Count of the contours.
            max_vertex_count: Maximum vertex Count.
            min_ratio: Minimum ratio of width to height.
            max_ratio: Maximum ratio of width to height.
        Returns:
            Contours as a list of numpy.ndarray.
        """
        output = []
        for contour in input_contours:
            x,y,w,h = cv2.boundingRect(contour)
            if (w < min_width or w > max_width):
                continue
            if (h < min_height or h > max_height):
                continue
            area = cv2.contourArea(contour)
            if (area < min_area):
                continue
            if (cv2.arcLength(contour, True) < min_perimeter):
                continue
            hull = cv2.convexHull(contour)
            solid = 100 * area / cv2.contourArea(hull)
            if (solid < solidity[0] or solid > solidity[1]):
                continue
            if (len(contour) < min_vertex_count or len(contour) > max_vertex_count):
                continue
            ratio = (float)(w) / h
            if (ratio < min_ratio or ratio > max_ratio):
                continue
            output.append(contour)
        return output


BlurType = Enum('BlurType', 'Box_Blur Gaussian_Blur Median_Filter Bilateral_Filter')


class DeliveryUndistorter:
    camera_matrix = numpy.array([[512.0486676110471, 0.0, 305.52035364138726], 
        [0.0, 515.3532387199512, 262.41020845383434], [0.0, 0.0, 1.0]])
    dist_coeffs = numpy.array([0.08454318371194253, -0.24818779001569383, 
        0.002509250787445395, 0.0033838430868377263, 0.12424667264216427])
    image_size = (640, 480)
    map_type = cv2.CV_32FC1 # CV_32FC1 or CV_16SC2
    alpha = 0
    interpolation = cv2.INTER_LINEAR

    def __init__(self):
        new_camera_matrix, validROI = \
        cv2.getOptimalNewCameraMatrix(self.camera_matrix, 
                                      self.dist_coeffs, 
                                      self.image_size, 
                                      self.alpha)
        print("Delivery All-Good ROI:", validROI)
        self.map1, self.map2 = \
            cv2.initUndistortRectifyMap(self.camera_matrix, 
                                        self.dist_coeffs, numpy.array([]), 
                                        new_camera_matrix,
                                        self.image_size, self.map_type)

    def undistort(self, frame):
        return cv2.remap(frame, self.map1, self.map2, self.interpolation)

class ExtraProcessingDelivery:
    """
    Performs extra processing on the pipeline's outputs and publishes data
    :param pipeline: the pipeline that just processed an image
    :return: None
    """
    # Camera constants
    horiz_FOV = 57.64 # ELP 57.64, LifeCam 25.18 * 2, XBox 41.97
    vert_FOV = 42.36 # ELP 42.36, LifeCam 52.696, XBox 32.67
    height = 35.75
    vert_angle = 0 # How far down the camera is pointed
    horiz_angle = 0 # How far to the right the camera is pointed
    horiz_offset = 0 # How far to the right the camera is shifted
    width_pixels = 640
    height_pixels = 480

    # Target constants
    target_bottom_height = 26.17519
    target_farthest_corner_height = 26.67595 # Height from ground to farthest left/right corners
    target_half_width = 7.355315 # Distance from center of pair to edge of target

    # Calculated constants
    half_height_pixels = height_pixels / 2
    half_width_pixels = width_pixels / 2
    horiz_tan = math.tan(math.radians(horiz_FOV/2))
    vert_tan = math.tan(math.radians(vert_FOV/2))
    target_half_width_squared = target_half_width**2
    target_width = target_half_width*2
    target_width_squared = target_width**2

    VisionTarget = namedtuple("VisionTarget", ["left_box", "right_box", "center"])
    TapeBox = namedtuple("TapeBox", ["bottom_left", "bottom_right", "top_left", "top_right"])

    tape_height = 5.82557 # Vertical, farthest bottom to farthest top
    top_bottom_height = 4.82405 # Vertical, left to right height
    tape_width = 3.31339 # Horizontal, farthest left to farthest right
    target_width = 14.7106299 # Horizontal, farthest left on left tape to farthest right on right target
    top_target_width = 11.8637795 # Horizontal, left top to right top distance
    bottom_target_width = 10.8468504 # Horizontal, left bottom to right bottom distance
    model_points = numpy.array([(-target_width/2, -top_bottom_height/2, 0), # Left Bottom Left
                                (-bottom_target_width/2, -tape_height/2, 0), # Left Bottom Right
                                (-top_target_width/2, tape_height/2, 0), # Left Top Left
                                (-4, top_bottom_height/2, 0), # Left Top Right
                                (bottom_target_width/2, -tape_height/2, 0), # Right Bottom Left
                                (target_width/2, -top_bottom_height/2, 0), # Right Bottom Right
                                (4, top_bottom_height/2, 0), # Right Top Left
                                (top_target_width/2, tape_height/2, 0)]) # Right Top Right



    def __init__(self, zmq_pub, undistorter):
        self._zmq_pub = zmq_pub
        self._undistorter = undistorter

    def _tilted_left(self, box):
        return box.top_right[0] < box.bottom_right[0] # Top right x less than bottom right x
    def _tilted_right(self, box):
        return box.top_left[0] > box.bottom_left[0] # Top left x greater than bottom left x
    def _find_center(self, box1, box2):
        """
        Take a (corner coords, arearect) and return (x, y)
        """
        return (((box2[1][0][0]+box2[1][1][0]/2)+(box1[1][0][0]+box1[1][1][0]/2))/2, \
        ((box2[1][0][1]+box2[1][1][1]/2)+(box1[1][0][1]+box1[1][1][1]/2))/2)
    def _calc_angle_h(self, pixel_x, distance):
        """
        Calculate angle to half_height_pixelspixel x coordinate. Distance to point required in case horiz_offset != 0.
        """
        angle_h =  (math.degrees(
                math.atan(((pixel_x-self.half_width_pixels)*self.horiz_tan
                /self.half_width_pixels)))) - self.horiz_angle
        if self.horiz_offset != 0:
            horiz_distance = math.tan(math.radians(angle_h)) * distance
            horiz_distance += self.horiz_offset
            angle_h = math.degrees(math.atan(horiz_distance/distance))
        return angle_h
    def _calc_distance(self, pixel_y, point_height=target_bottom_height):
        """
        Calculate distance and angle v. Returns (distance, angle_v)
        """
        angle_v = (math.degrees(
                math.atan(((pixel_y-self.half_height_pixels)*-1*self.vert_tan
                /self.half_height_pixels)))) - self.vert_angle
        distance = abs(point_height-self.height) / math.tan(math.radians(abs(angle_v)))
        return (distance, angle_v)

    def _gen_tape_box(self, box_points):
        sorted_box = sorted(box_points, key=lambda point: point[1]) # Y pos sort
        top_points = sorted_box[0:2] # Top two points
        bottom_points = sorted_box[2:4] # Bottom two points
        top_points.sort(key=lambda point: point[0]) # X pos sort
        bottom_points.sort(key=lambda point: point[0]) # X pos soft
        return self.TapeBox(bottom_left=bottom_points[0], 
            bottom_right=bottom_points[1], top_left=top_points[0],
            top_right=top_points[1])

    def _gen_boxes(self, contours):
        boxes = [cv2.minAreaRect(contour) for contour in contours]
        # minAreaRect returns ((x, y), (width, height), angle), angle is -90 to 0
        # see https://stackoverflow.com/questions/15956124/minarearect-angles-unsure-about-the-angle-returned
        return [[cv2.boxPoints(box), box] for box in boxes] # Create [corner coords, arearect] lists


    def _target_calc(self, target, time):
        # Average the y of the lowest in frame (max y) corner coords
        # max returns (x,y) so get [1]
        lower_center_y = (max(target.left_box[0], key=lambda point: point[1])[1] + \
        max(target.right_box[0], key=lambda point: point[1])[1])/2

        distance, angle_v = self._calc_distance(lower_center_y)
        angle_h_robot = self._calc_angle_h(target.center[0], distance)
        print("Angle V:", angle_v)
        print("Distance:", distance)
        print("Angle:", angle_h_robot)
        # Find target angle from robot perspective to each edge of the pair
        distance_right_edge = self._calc_distance(target.right_box[0][3][1], \
        self.target_farthest_corner_height)[0]
        distance_left_edge = self._calc_distance(target.left_box[0][0][1], \
        self.target_farthest_corner_height)[0]
        print("Y L:", target.left_box[0][0][1])
        print("Y R:", target.right_box[0][3][1])
        print("Distance L:", distance_left_edge)
        print("Distance R:", distance_right_edge)
        # Law of cosines: https://www.mathsisfun.com/algebra/trig-cosine-law.html
        # Trying to find B here using cos(B) = (c^2+a^2-b^2)/2ca
        # Where b is distance_right_edge, a is target_center_to_edge, c is distance_left_edge
        target_angle_left_cos = (distance_left_edge**2 + self.target_width_squared - distance_right_edge**2) / \
        (2*distance_left_edge*self.target_width)
        # Swap which is b and c to find angle to right corner
        target_angle_right_cos = (distance_right_edge**2 + self.target_width_squared - distance_left_edge**2) / \
        (2*distance_right_edge*self.target_width)
        # Angle directly to center from left edge
        target_angle_cos = (distance**2 + self.target_half_width_squared - distance_left_edge**2) / \
        (2*distance*self.target_half_width)
        # interact(local=locals())
        target_angle = math.acos(target_angle_cos)
        target_angle_left = math.acos(target_angle_left_cos)
        target_angle_right = math.acos(target_angle_right_cos)
        print("Target Angle:", math.degrees(target_angle))
        print("Target Angle L:", math.degrees(target_angle_left))
        print("Target Angle R:", math.degrees(target_angle_right))
        self._zmq_pub.zmqPubDoubles("distangle", time, distance, angle_h_robot, target_angle)

    def _target_calc_solvepnp(self, target, time):
        image_points = numpy.array([target.left_box[0].bottom_left,
                                    target.left_box[0].bottom_right,
                                    target.left_box[0].top_left,
                                    target.left_box[0].top_right,
                                    target.right_box[0].bottom_left,
                                    target.right_box[0].bottom_right,
                                    target.right_box[0].top_left,
                                    target.right_box[0].top_right])
        success, rotation_vector, translation_vector = \
            cv2.solvePnP(self.model_points, image_points, 
                        self._undistorter.camera_matrix, 
                        self._undistorter.dist_coeffs, 
                        flags=cv2.SOLVEPNP_ITERATIVE)
        print("Model Points:", self.model_points)
        print("Image Points:", image_points)
        if success:
            print("Translation:", translation_vector)
            x = translation_vector[0][0]
            z = translation_vector[2][0]
            z2 = math.sin(self.vert_angle) * translation_vector[1][0] + math.cos(self.vert_angle) * translation_vector[2][0]
            # distance in the horizontal plane between camera and target
            distance = math.sqrt(x**2 + z**2)
            distance2 = math.sqrt(x**2 + z2**2)

            # horizontal angle between camera center line and target
            angle1 = math.atan2(x, z)
            rot, _ = cv2.Rodrigues(rotation_vector)
            rot_inv = rot.transpose()
            pzero_world = numpy.matmul(rot_inv, -translation_vector)
            angle2 = math.atan2(pzero_world[0][0], pzero_world[2][0])
            xoffset = math.sin(angle2)*distance
            zoffset = math.sqrt(distance**2 - xoffset**2)
            angle_world = numpy.degrees((angle1 + angle2) * -1)
            #print(pzero_world[0][0], pzero_world[2][0])
            print("Distance: ", distance, "Distance Adjusted: ", distance2, 
                  "Angle1: ", numpy.degrees(angle1), "Angle 2: ", 
                  numpy.degrees(angle2), "\r\nX Offset: ", xoffset, 
                  "Z Offset: ", zoffset, "World Angle:", 
                  angle_world, "\n")
            self._zmq_pub.zmqPubDoubles("coordinates", time, xoffset, zoffset, 
                                        angle_world)
        else:
            print("SolvePnP failed")

    def process(self, pipeline, time):
        boxes = self._gen_boxes(pipeline.convex_hulls_output)
        boxes.sort(key=lambda box: box[1][0][0]) # Sort by x position
        targets = []
        # Iterate over pairs of boxes (saving to VisionTargets) and find centers
        for first, second in zip(boxes[::2], boxes[1::2]):
            targets.append(self.VisionTarget(first, second, center = \
            self._find_center(first, second)))
        # Other set of combinations
        # Not all combinations will be valid, that is expected
        for first, second in zip(boxes[1::2], boxes[2::2]):
            targets.append(self.VisionTarget(first, second, center = \
            self._find_center(first, second)))
        # Sort coordinates within each box to [left, bottom, top, right] in a TapeBox
        for target in targets:
            target.left_box[0] = self._gen_tape_box(target.left_box[0])
            target.right_box[0] = self._gen_tape_box(target.right_box[0])
        # Filter out target possibilities without correct tilts
        targets = list(filter(lambda target: self._tilted_right(target.left_box[0]) and \
        self._tilted_left(target.right_box[0]), targets))
        # Find target with lowest y value (closest to camera)
        # targets.sort(key=lambda target: target.center[1], reverse=True)
        # Pick target closest to center of frame (x), may need to use angle_h if off center camera
        targets.sort(key = lambda target: abs(target.center[0]-self.half_width_pixels))
        print([abs(target.center[0]-self.half_width_pixels) for target in targets])
        try:
            target = targets[0]
        except IndexError:
            print("No valid targets found")
            return
        self._target_calc_solvepnp(target, time)


class ExtraProcessingHatch:
    # Camera constants
    horiz_FOV = 25.18 * 2
    vert_FOV = 52.696
    height = 31
    vert_angle = 52 # How far down the camera is pointed
    horiz_angle = 0 # How far to the right the camera is pointed
    horiz_offset = 0 # How far to the right the camera is shifted
    width_pixels = 160
    height_pixels = 120

    # Calculated constants
    half_height_pixels = height_pixels / 2
    half_width_pixels = width_pixels / 2
    horiz_tan = math.tan(math.radians(horiz_FOV/2))
    vert_tan = math.tan(math.radians(vert_FOV/2))

    def __init__(self, zmq_pub):
        self._zmq_pub = zmq_pub

    def process(self, pipeline, time):
        contours = pipeline.filter_contours_output
        contours.sort(key=cv2.contourArea) # Find largest area contour
        try:
            x, y, w, h = cv2.boundingRect(contours[0])
            center = ((x+(w/2)), (y+(h/2)));

            angle_h = (math.degrees(
                        math.atan(((center[0]-self.half_width_pixels)*self.horiz_tan
                        /self.half_width_pixels)))) - self.horiz_angle
            angle_v = (math.degrees(
                        math.atan(((center[1]-self.half_height_pixels)*-1*self.vert_tan
                        /self.half_height_pixels)))) - self.vert_angle
            distance = math.tan(math.radians(90-abs(angle_v))) * self.height
            if self.horiz_offset != 0:
                horiz_distance = math.tan(math.radians(angle_h)) * distance
                horiz_distance += self.horiz_offset
                angle_h = math.degrees(math.atan(horiz_distance/distance))
            self._zmq_pub.zmqPubDoubles("distangle", time, distance, angle_h)
            print("Distance:", distance)
            print("Angle:", angle_h)
        except IndexError:
            # No contours found
            print("Ran pipeline but no contours")


class ExtraProcessingWhiteTape(ExtraProcessingDelivery):
    # Camera constants
    horiz_FOV = 57.64 # ELP 57.64, LifeCam 25.18 * 2, XBox 41.97
    vert_FOV = 42.36 # ELP 42.36, LifeCam 52.696, XBox 32.67
    height = 34.25
    vert_angle = 47 # How far down the camera is pointed
    horiz_angle = 0 # How far to the right the camera is pointed
    horiz_offset = 0 # How far to the right the camera is shifted
    width_pixels = 320
    height_pixels = 240

    # Calculated constants
    half_height_pixels = height_pixels / 2
    half_width_pixels = width_pixels / 2
    horiz_tan = math.tan(math.radians(horiz_FOV/2))
    vert_tan = math.tan(math.radians(vert_FOV/2))

    # len of line sqrt((x2-x1)^2 + (y2-y1))^2)
    def _line_len(self, point1, point2): 
        return numpy.sqrt((point2[0]-point1[0])**2 + (point2[1] - point1[1]) **2)

    def __init__(self, zmq_pub):
        self._zmq_pub = zmq_pub

    def process(self, pipeline, time):
        boxes = self._gen_boxes(pipeline.filter_contours_output)
        boxes = [[self._gen_tape_box(box[0]), box[1]] for box in boxes]
        # Find box with longest left-top distance (tape on floor has longest edge)
        boxes.sort(key=lambda box: max(self._line_len(box[0].left, box[0].top), 
            self._line_len(box[0].right, box[0].top)), reverse=True)
        try:
            point1 = boxes[0][0].top
            # Point 2 is the second highest point (could be right or left depending on angle)
            point2 = sorted(boxes[0][0], key=lambda point: point[1])[1]
            # Average the two far end of tape points (point1 and point2) to get the center of tape end point
            point = ((point1[0] + point2[0])/2, (point1[1] + point2[1])/2)
            print("Point:", point)
            # Tape end is on ground and _calc_distance default point height is for retro tape
            distance = self._calc_distance(point[1], point_height=0)[0]
            angle_h = self._calc_angle_h(point[0], distance)
            print("Angle:", angle_h)
            print("Distance", distance)
            self._zmq_pub.zmqPubDoubles("distangle", time, distance, angle_h)
        except IndexError:
            print("No tape pieces found")


def getTimeMS():
    return int(round(time.time() * 1000))

class ZmqPubIF:
    """
    A simple zmq wrapper class that provides publishing functionality
    """
    def __init__(self, port):
        self.socket = context.socket(zmq.PUB)
        self.socket.bind("tcp://*:%s" % port)

    def zmqPubStr(self, topic, dataStr):
        # The topic is the first string sent
        self.socket.send_string(topic, zmq.SNDMORE)
        # Here we're assuming we're just sending a string as payload
        self.socket.send_string(dataStr)

    def zmqPubDoubles(self, topic, *doubles):
        self.socket.send_string(topic, zmq.SNDMORE)
        for double in doubles[:-1]:
            self.socket.send(struct.pack("!d", double), zmq.SNDMORE)
        self.socket.send(struct.pack("!d", doubles[-1]))

class ZmqRecvIF:
    """
    A simple class to recieve commands over ZeroMQ
    """
    def __init__(self, port):
        self.socket = context.socket(zmq.PULL)
        self.socket.bind("tcp://*:%s" % port)
        self.flags = 0

    def recv_command(self):
        return self.socket.recv_multipart(self.flags)

    def set_blocking(self, block):
        self.flags = 0 if block else zmq.NOBLOCK


class RioTimer:
    """
    A class to return a time that matches the roboRIO
    """
    camera_latency = 0.131272727
    # How long it takes between the rio sending the time and set_rio_timer being called
    network_latency = 0.01 # Guess

    def set_rio_timer(self, time):
        self._rio_time_offset = time + self.network_latency - timeit.default_timer()

    def get_time(self):
        return timeit.default_timer() + self._rio_time_offset - \
               self.camera_latency


context = zmq.Context()
def main():
    zmq_publish_port = "5556"
    zmq_recv_port = "5555"
    Pipeline = namedtuple("Pipeline", ["GRIP_pipeline", "processing_class", \
    "camera", "undistorter"])

    print('Initializing ZMQ Publisher')
    zmq_pub = ZmqPubIF(zmq_publish_port)

    print('Initializing ZMQ Reciever')
    zmq_recv = ZmqRecvIF(zmq_recv_port)

    timer = RioTimer()

    print('Creating pipelines')
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
    # cap.set(cv2.CAP_PROP_BRIGHTNESS, 0) # Only needed on Logitech (maybe)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, 30)
    cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0.25) # Disable auto exposure
    # cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0.75) # Enable auto exposure
    cap.set(cv2.CAP_PROP_EXPOSURE, 0.002) # Minimum exposure, should be 0 on LifeCam/XBox, 0.002 on new ELP, 2 on Logitech (have had problems on Logitech)
    # cap.set(cv2.CAP_PROP_SHARPNESS, 0) # Removes haloing on ELP, not needed on LifeCam or XBox
    delivery_undistorter = DeliveryUndistorter()
    pipelines = {b"none" : None, \
    b"delivery" : Pipeline(GripPipelineRetro(), \
    ExtraProcessingDelivery(zmq_pub, delivery_undistorter), cap, None), # Undistorter removed for now for SolvePnP testing
    b"hatch" : Pipeline(GripPipelineHatch(), \
    ExtraProcessingHatch(zmq_pub), cap, None),
    b"whitetape" : Pipeline(GripPipelineWhiteTape(), \
        ExtraProcessingWhiteTape(zmq_pub), cap, delivery_undistorter)}
    pipeline = None

    print('Running pipeline')
    while cap.isOpened():
        try:
            zmq_command = zmq_recv.recv_command()
            print("Recieved command", zmq_command[0])
            # print("Additionad frames:", zmq_command[1:])
            if zmq_command[0] == b"set_pipeline":
                print("Set pipeline to {}".format(zmq_command[1].decode()))
                pipeline = pipelines[zmq_command[1]]
                if pipeline is not None:
                    zmq_recv.set_blocking(False)
                    pipeline.camera.grab() # Flush the 1 frame that could be in the buffer
                else:
                    zmq_recv.set_blocking(True) # Block while waiting for commands when not running a pipeline
            elif zmq_command[0] == b"write_frame":
                if pipeline is not None:
                    cv2.imwrite("capture.jpg", pipeline.camera.read()[1])
                    print("Saved frame to capture.jpg")
                else:
                    cv2.imwrite("capture.jpg", cap.read()[1])
                    print("Saved frame to capture.jpg using default camera")
            elif zmq_command[0] == b"set_time":
                timer.set_rio_timer(struct.unpack("!d", zmq_command[1])[0])
        except zmq.ZMQError:
            # No command
            pass
        if pipeline is not None:
            have_frame, frame = pipeline.camera.read()
            time = timer.get_time()
            try:
                frame = pipeline.undistorter.undistort(frame)
                pass
            except AttributeError:
                # There is no undistorter
                pass
            if have_frame:
                pipeline.GRIP_pipeline.process(frame)
                pipeline.processing_class.process(pipeline.GRIP_pipeline, time)

    print('Capture closed')


if __name__ == '__main__':
    main()
