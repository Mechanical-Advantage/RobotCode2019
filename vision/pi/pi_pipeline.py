#!/usr/bin/python3

"""
This program uses a generated GRIP pipeline for object tracking.

It performs calculations on the resulting geometry and publishes the results 
through ZeroMQ for use by the RoboRio.
"""
import cv2
import numpy
import math
from enum import Enum
import time
import zmq
import struct
from collections import namedtuple
from code import interact

# Retro reflective tape pipeline
class GripPipelineRetro:
    """
    An OpenCV pipeline generated by GRIP.
    """
    
    def __init__(self):
        """initializes all values to presets or None if need to be set
        """

        self.__resize_image_width = 160.0
        self.__resize_image_height = 120.0
        self.__resize_image_interpolation = cv2.INTER_LINEAR

        self.resize_image_output = None

        self.__hsv_threshold_input = self.resize_image_output
        self.__hsv_threshold_hue = [57, 78]
        self.__hsv_threshold_saturation = [15, 255.0]
        self.__hsv_threshold_value = [112, 255]

        self.hsv_threshold_output = None

        self.__find_contours_input = self.hsv_threshold_output
        self.__find_contours_external_only = True

        self.find_contours_output = None

        self.__filter_contours_contours = self.find_contours_output
        self.__filter_contours_min_area = 20.0
        self.__filter_contours_min_perimeter = 30.0
        self.__filter_contours_min_width = 0.0
        self.__filter_contours_max_width = 1000.0
        self.__filter_contours_min_height = 0.0
        self.__filter_contours_max_height = 1000.0
        self.__filter_contours_solidity = [79, 100]
        self.__filter_contours_max_vertices = 1000000.0
        self.__filter_contours_min_vertices = 0.0
        self.__filter_contours_min_ratio = 0.0
        self.__filter_contours_max_ratio = 1000.0

        self.filter_contours_output = None

        self.__convex_hulls_contours = self.filter_contours_output

        self.convex_hulls_output = None


    def process(self, source0):
        """
        Runs the pipeline and sets all outputs to new values.
        """
        # Step Resize_Image0:
        #self.__resize_image_input = source0
        #(self.resize_image_output) = self.__resize_image(self.__resize_image_input, self.__resize_image_width, self.__resize_image_height, self.__resize_image_interpolation)

        # Step HSV_Threshold0:
        self.__hsv_threshold_input = source0
        (self.hsv_threshold_output) = self.__hsv_threshold(self.__hsv_threshold_input, self.__hsv_threshold_hue, self.__hsv_threshold_saturation, self.__hsv_threshold_value)

        # Step Find_Contours0:
        self.__find_contours_input = self.hsv_threshold_output
        (self.find_contours_output) = self.__find_contours(self.__find_contours_input, self.__find_contours_external_only)

        # Step Filter_Contours0:
        self.__filter_contours_contours = self.find_contours_output
        (self.filter_contours_output) = self.__filter_contours(self.__filter_contours_contours, self.__filter_contours_min_area, self.__filter_contours_min_perimeter, self.__filter_contours_min_width, self.__filter_contours_max_width, self.__filter_contours_min_height, self.__filter_contours_max_height, self.__filter_contours_solidity, self.__filter_contours_max_vertices, self.__filter_contours_min_vertices, self.__filter_contours_min_ratio, self.__filter_contours_max_ratio)

        # Step Convex_Hulls0:
        self.__convex_hulls_contours = self.filter_contours_output
        (self.convex_hulls_output) = self.__convex_hulls(self.__convex_hulls_contours)


    @staticmethod
    def __resize_image(input, width, height, interpolation):
        """Scales and image to an exact size.
        Args:
            input: A numpy.ndarray.
            Width: The desired width in pixels.
            Height: The desired height in pixels.
            interpolation: Opencv enum for the type fo interpolation.
        Returns:
            A numpy.ndarray of the new size.
        """
        return cv2.resize(input, ((int)(width), (int)(height)), 0, 0, interpolation)

    @staticmethod
    def __hsv_threshold(input, hue, sat, val):
        """Segment an image based on hue, saturation, and value ranges.
        Args:
            input: A BGR numpy.ndarray.
            hue: A list of two numbers the are the min and max hue.
            sat: A list of two numbers the are the min and max saturation.
            lum: A list of two numbers the are the min and max value.
        Returns:
            A black and white numpy.ndarray.
        """
        out = cv2.cvtColor(input, cv2.COLOR_BGR2HSV)
        return cv2.inRange(out, (hue[0], sat[0], val[0]),  (hue[1], sat[1], val[1]))

    @staticmethod
    def __find_contours(input, external_only):
        """Sets the values of pixels in a binary image to their distance to the nearest black pixel.
        Args:
            input: A numpy.ndarray.
            external_only: A boolean. If true only external contours are found.
        Return:
            A list of numpy.ndarray where each one represents a contour.
        """
        if(external_only):
            mode = cv2.RETR_EXTERNAL
        else:
            mode = cv2.RETR_LIST
        method = cv2.CHAIN_APPROX_SIMPLE
        im2, contours, hierarchy =cv2.findContours(input, mode=mode, method=method)
        return contours

    @staticmethod
    def __filter_contours(input_contours, min_area, min_perimeter, min_width, max_width,
                        min_height, max_height, solidity, max_vertex_count, min_vertex_count,
                        min_ratio, max_ratio):
        """Filters out contours that do not meet certain criteria.
        Args:
            input_contours: Contours as a list of numpy.ndarray.
            min_area: The minimum area of a contour that will be kept.
            min_perimeter: The minimum perimeter of a contour that will be kept.
            min_width: Minimum width of a contour.
            max_width: MaxWidth maximum width.
            min_height: Minimum height.
            max_height: Maximimum height.
            solidity: The minimum and maximum solidity of a contour.
            min_vertex_count: Minimum vertex Count of the contours.
            max_vertex_count: Maximum vertex Count.
            min_ratio: Minimum ratio of width to height.
            max_ratio: Maximum ratio of width to height.
        Returns:
            Contours as a list of numpy.ndarray.
        """
        output = []
        for contour in input_contours:
            x,y,w,h = cv2.boundingRect(contour)
            if (w < min_width or w > max_width):
                continue
            if (h < min_height or h > max_height):
                continue
            area = cv2.contourArea(contour)
            if (area < min_area):
                continue
            if (cv2.arcLength(contour, True) < min_perimeter):
                continue
            hull = cv2.convexHull(contour)
            solid = 100 * area / cv2.contourArea(hull)
            if (solid < solidity[0] or solid > solidity[1]):
                continue
            if (len(contour) < min_vertex_count or len(contour) > max_vertex_count):
                continue
            ratio = (float)(w) / h
            if (ratio < min_ratio or ratio > max_ratio):
                continue
            output.append(contour)
        return output

    @staticmethod
    def __convex_hulls(input_contours):
        """Computes the convex hulls of contours.
        Args:
            input_contours: A list of numpy.ndarray that each represent a contour.
        Returns:
            A list of numpy.ndarray that each represent a contour.
        """
        output = []
        for contour in input_contours:
            output.append(cv2.convexHull(contour))
        return output

class GripPipelineHatch:
    """
    An OpenCV pipeline generated by GRIP.
    """
    
    def __init__(self):
        """initializes all values to presets or None if need to be set
        """

        self.__resize_image_width = 160.0
        self.__resize_image_height = 120.0
        self.__resize_image_interpolation = cv2.INTER_CUBIC

        self.resize_image_output = None

        self.__blur_input = self.resize_image_output
        self.__blur_type = BlurType.Box_Blur
        self.__blur_radius = 1.4414422146908874

        self.blur_output = None

        self.__hsv_threshold_input = self.blur_output
        self.__hsv_threshold_hue = [22.66187050359711, 40.546076257074276]
        self.__hsv_threshold_saturation = [29.352516236065103, 156.65527184261805]
        self.__hsv_threshold_value = [150.43164789676666, 255.0]

        self.hsv_threshold_output = None

        self.__find_contours_input = self.hsv_threshold_output
        self.__find_contours_external_only = True

        self.find_contours_output = None

        self.__convex_hulls_contours = self.find_contours_output

        self.convex_hulls_output = None

        self.__filter_contours_contours = self.convex_hulls_output
        self.__filter_contours_min_area = 150.0
        self.__filter_contours_min_perimeter = 0.0
        self.__filter_contours_min_width = 0.0
        self.__filter_contours_max_width = 1000.0
        self.__filter_contours_min_height = 0.0
        self.__filter_contours_max_height = 1000.0
        self.__filter_contours_solidity = [0.0, 100.0]
        self.__filter_contours_max_vertices = 1000000.0
        self.__filter_contours_min_vertices = 12.0
        self.__filter_contours_min_ratio = 0.0
        self.__filter_contours_max_ratio = 1000.0

        self.filter_contours_output = None


    def process(self, source0):
        """
        Runs the pipeline and sets all outputs to new values.
        """
        # Step Resize_Image0:
        self.__resize_image_input = source0
        (self.resize_image_output) = self.__resize_image(self.__resize_image_input, self.__resize_image_width, self.__resize_image_height, self.__resize_image_interpolation)

        # Step Blur0:
        self.__blur_input = self.resize_image_output
        (self.blur_output) = self.__blur(self.__blur_input, self.__blur_type, self.__blur_radius)

        # Step HSV_Threshold0:
        self.__hsv_threshold_input = self.blur_output
        (self.hsv_threshold_output) = self.__hsv_threshold(self.__hsv_threshold_input, self.__hsv_threshold_hue, self.__hsv_threshold_saturation, self.__hsv_threshold_value)

        # Step Find_Contours0:
        self.__find_contours_input = self.hsv_threshold_output
        (self.find_contours_output) = self.__find_contours(self.__find_contours_input, self.__find_contours_external_only)

        # Step Convex_Hulls0:
        self.__convex_hulls_contours = self.find_contours_output
        (self.convex_hulls_output) = self.__convex_hulls(self.__convex_hulls_contours)

        # Step Filter_Contours0:
        self.__filter_contours_contours = self.convex_hulls_output
        (self.filter_contours_output) = self.__filter_contours(self.__filter_contours_contours, self.__filter_contours_min_area, self.__filter_contours_min_perimeter, self.__filter_contours_min_width, self.__filter_contours_max_width, self.__filter_contours_min_height, self.__filter_contours_max_height, self.__filter_contours_solidity, self.__filter_contours_max_vertices, self.__filter_contours_min_vertices, self.__filter_contours_min_ratio, self.__filter_contours_max_ratio)


    @staticmethod
    def __resize_image(input, width, height, interpolation):
        """Scales and image to an exact size.
        Args:
            input: A numpy.ndarray.
            Width: The desired width in pixels.
            Height: The desired height in pixels.
            interpolation: Opencv enum for the type fo interpolation.
        Returns:
            A numpy.ndarray of the new size.
        """
        return cv2.resize(input, ((int)(width), (int)(height)), 0, 0, interpolation)

    @staticmethod
    def __blur(src, type, radius):
        """Softens an image using one of several filters.
        Args:
            src: The source mat (numpy.ndarray).
            type: The blurType to perform represented as an int.
            radius: The radius for the blur as a float.
        Returns:
            A numpy.ndarray that has been blurred.
        """
        if(type is BlurType.Box_Blur):
            ksize = int(2 * round(radius) + 1)
            return cv2.blur(src, (ksize, ksize))
        elif(type is BlurType.Gaussian_Blur):
            ksize = int(6 * round(radius) + 1)
            return cv2.GaussianBlur(src, (ksize, ksize), round(radius))
        elif(type is BlurType.Median_Filter):
            ksize = int(2 * round(radius) + 1)
            return cv2.medianBlur(src, ksize)
        else:
            return cv2.bilateralFilter(src, -1, round(radius), round(radius))

    @staticmethod
    def __hsv_threshold(input, hue, sat, val):
        """Segment an image based on hue, saturation, and value ranges.
        Args:
            input: A BGR numpy.ndarray.
            hue: A list of two numbers the are the min and max hue.
            sat: A list of two numbers the are the min and max saturation.
            lum: A list of two numbers the are the min and max value.
        Returns:
            A black and white numpy.ndarray.
        """
        out = cv2.cvtColor(input, cv2.COLOR_BGR2HSV)
        return cv2.inRange(out, (hue[0], sat[0], val[0]),  (hue[1], sat[1], val[1]))

    @staticmethod
    def __find_contours(input, external_only):
        """Sets the values of pixels in a binary image to their distance to the nearest black pixel.
        Args:
            input: A numpy.ndarray.
            external_only: A boolean. If true only external contours are found.
        Return:
            A list of numpy.ndarray where each one represents a contour.
        """
        if(external_only):
            mode = cv2.RETR_EXTERNAL
        else:
            mode = cv2.RETR_LIST
        method = cv2.CHAIN_APPROX_SIMPLE
        im2, contours, hierarchy =cv2.findContours(input, mode=mode, method=method)
        return contours

    @staticmethod
    def __convex_hulls(input_contours):
        """Computes the convex hulls of contours.
        Args:
            input_contours: A list of numpy.ndarray that each represent a contour.
        Returns:
            A list of numpy.ndarray that each represent a contour.
        """
        output = []
        for contour in input_contours:
            output.append(cv2.convexHull(contour))
        return output

    @staticmethod
    def __filter_contours(input_contours, min_area, min_perimeter, min_width, max_width,
                        min_height, max_height, solidity, max_vertex_count, min_vertex_count,
                        min_ratio, max_ratio):
        """Filters out contours that do not meet certain criteria.
        Args:
            input_contours: Contours as a list of numpy.ndarray.
            min_area: The minimum area of a contour that will be kept.
            min_perimeter: The minimum perimeter of a contour that will be kept.
            min_width: Minimum width of a contour.
            max_width: MaxWidth maximum width.
            min_height: Minimum height.
            max_height: Maximimum height.
            solidity: The minimum and maximum solidity of a contour.
            min_vertex_count: Minimum vertex Count of the contours.
            max_vertex_count: Maximum vertex Count.
            min_ratio: Minimum ratio of width to height.
            max_ratio: Maximum ratio of width to height.
        Returns:
            Contours as a list of numpy.ndarray.
        """
        output = []
        for contour in input_contours:
            x,y,w,h = cv2.boundingRect(contour)
            if (w < min_width or w > max_width):
                continue
            if (h < min_height or h > max_height):
                continue
            area = cv2.contourArea(contour)
            if (area < min_area):
                continue
            if (cv2.arcLength(contour, True) < min_perimeter):
                continue
            hull = cv2.convexHull(contour)
            solid = 100 * area / cv2.contourArea(hull)
            if (solid < solidity[0] or solid > solidity[1]):
                continue
            if (len(contour) < min_vertex_count or len(contour) > max_vertex_count):
                continue
            ratio = (float)(w) / h
            if (ratio < min_ratio or ratio > max_ratio):
                continue
            output.append(contour)
        return output


BlurType = Enum('BlurType', 'Box_Blur Gaussian_Blur Median_Filter Bilateral_Filter')


class DeliveryUndistorter:
    camera_matrix = numpy.array([[3.1788942578748737e+02, 0, 160],
    [0, 3.1788942578748737e+02, 120], [0, 0, 1]])
    dist_coeffs = numpy.array([-4.1836983489089313e-01, 
    3.4929300225135756e-01, 0, 0, 4.5534003180269961e-01])
    image_size = (320, 240)
    map_type = cv2.CV_32FC1 # CV_32FC1 or CV_16SC2
    alpha = 1
    interpolation = cv2.INTER_LINEAR

    def __init__(self):
        new_camera_matrix, validROI = \
        cv2.getOptimalNewCameraMatrix(self.camera_matrix, 
                                      self.dist_coeffs, 
                                      self.image_size, 
                                      self.alpha)
        print("Delivery All-Good ROI:", validROI)
        self.map1, self.map2 = \
            cv2.initUndistortRectifyMap(self.camera_matrix, 
                                        self.dist_coeffs, numpy.array([]), 
                                        new_camera_matrix,
                                        self.image_size, self.map_type)

    def undistort(self, frame):
        return cv2.remap(frame, self.map1, self.map2, self.interpolation)

class ExtraProcessingDelivery:
    """
    Performs extra processing on the pipeline's outputs and publishes data
    :param pipeline: the pipeline that just processed an image
    :return: None
    """
    # Camera constants
    horiz_FOV = 57.64 # ELP 57.64, LifeCam 25.18 * 2, XBox 41.97
    vert_FOV = 42.36 # ELP 42.36, LifeCam 52.696, XBox 32.67
    height = 36
    vert_angle = 21 # How far down the camera is pointed
    horiz_angle = 0 # How far to the right the camera is pointed
    horiz_offset = 0 # How far to the right the camera is shifted
    width_pixels = 320
    height_pixels = 240

    # Target constants
    target_bottom_height = 26.17519
    target_farthest_corner_height = 26.67595 # Height from ground to farthest left/right corners
    target_half_width = 7.355315 # Distance from center of pair to edge of target

    # Calculated constants
    half_height_pixels = height_pixels / 2
    half_width_pixels = width_pixels / 2
    horiz_tan = math.tan(math.radians(horiz_FOV/2))
    vert_tan = math.tan(math.radians(vert_FOV/2))
    target_half_width_squared = target_half_width**2
    target_width = target_half_width*2
    target_width_squared = target_width**2

    VisionTarget = namedtuple("VisionTarget", ["left_box", "right_box", "center"])
    TapeBox = namedtuple("TapeBox", ["left", "bottom", "top", "right"])

    tape_height = 5.82557 # Vertical, farthest bottom to farthest top
    top_bottom_height = 4.82405 # Vertical, left to right height
    tape_width = 3.31339 # Horizontal, farthest left to farthest right
    target_width = 14.7106299 # Horizontal, farthest left on left tape to farthest right on right target
    top_target_width = 11.8637795 # Horizontal, left top to right top distance
    bottom_target_width = 10.8468504 # Horizontal, left bottom to right bottom distance
    model_points = numpy.array([(0, -target_width/2, -top_bottom_height/2), # Left Left
                                (0, -bottom_target_width/2, -tape_height/2), # Left Bottom
                                (0, -top_target_width/2, tape_height/2), # Left Top
                                (0, -4, top_bottom_height/2), # Left Right
                                (0, 4, top_bottom_height/2), # Right Left
                                (0, bottom_target_width/2, -tape_height/2), # Right Bottom
                                (0, top_target_width/2, tape_height/2), # Right Top
                                (0, target_width/2, -top_bottom_height/2)]) # Right Right


    def __init__(self, zmq_pub, undistorter):
        self._zmq_pub = zmq_pub
        self._undistorter = undistorter

    def _tilted_left(self, box):
        return box.left[1] < box.right[1] # 1st point y above 4th point y
    def _tilted_right(self, box):
        return box.right[1] < box.left[1] # 4th point y above 1st point y
    def _find_center(self, box1, box2):
        """
        Take a (corner coords, arearect) and return (x, y)
        """
        return (((box2[1][0][0]+box2[1][1][0]/2)+(box1[1][0][0]+box1[1][1][0]/2))/2, \
        ((box2[1][0][1]+box2[1][1][1]/2)+(box1[1][0][1]+box1[1][1][1]/2))/2)
    def _calc_angle_h(self, pixel_x, distance):
        """
        Calculate angle to half_height_pixelspixel x coordinate. Distance to point required in case horiz_offset != 0.
        """
        angle_h =  (math.degrees(
                math.atan(((pixel_x-self.half_width_pixels)*self.horiz_tan
                /self.half_width_pixels)))) - self.horiz_angle
        if self.horiz_offset != 0:
            horiz_distance = math.tan(math.radians(angle_h)) * distance
            horiz_distance += self.horiz_offset
            angle_h = math.degrees(math.atan(horiz_distance/distance))
        return angle_h
    def _calc_distance(self, pixel_y, point_height=target_bottom_height):
        """
        Calculate distance and angle v. Returns (distance, angle_v)
        """
        angle_v = (math.degrees(
                math.atan(((pixel_y-self.half_height_pixels)*-1*self.vert_tan
                /self.half_height_pixels)))) - self.vert_angle
        distance = abs(point_height-self.height) / math.tan(math.radians(abs(angle_v)))
        return (distance, angle_v)

    def _target_calc(self, target):
        # Average the y of the lowest in frame (max y) corner coords
        # max returns (x,y) so get [1]
        lower_center_y = (max(target.left_box[0], key=lambda point: point[1])[1] + \
        max(target.right_box[0], key=lambda point: point[1])[1])/2

        distance, angle_v = self._calc_distance(lower_center_y)
        angle_h_robot = self._calc_angle_h(target.center[0], distance)
        print("Angle V:", angle_v)
        print("Distance:", distance)
        print("Angle:", angle_h_robot)
        # Find target angle from robot perspective to each edge of the pair
        distance_right_edge = self._calc_distance(target.right_box[0][3][1], \
        self.target_farthest_corner_height)[0]
        distance_left_edge = self._calc_distance(target.left_box[0][0][1], \
        self.target_farthest_corner_height)[0]
        print("Y L:", target.left_box[0][0][1])
        print("Y R:", target.right_box[0][3][1])
        print("Distance L:", distance_left_edge)
        print("Distance R:", distance_right_edge)
        # Law of cosines: https://www.mathsisfun.com/algebra/trig-cosine-law.html
        # Trying to find B here using cos(B) = (c^2+a^2-b^2)/2ca
        # Where b is distance_right_edge, a is target_center_to_edge, c is distance_left_edge
        target_angle_left_cos = (distance_left_edge**2 + self.target_width_squared - distance_right_edge**2) / \
        (2*distance_left_edge*self.target_width)
        # Swap which is b and c to find angle to right corner
        target_angle_right_cos = (distance_right_edge**2 + self.target_width_squared - distance_left_edge**2) / \
        (2*distance_right_edge*self.target_width)
        # Angle directly to center from left edge
        target_angle_cos = (distance**2 + self.target_half_width_squared - distance_left_edge**2) / \
        (2*distance*self.target_half_width)
        # interact(local=locals())
        target_angle = math.acos(target_angle_cos)
        target_angle_left = math.acos(target_angle_left_cos)
        target_angle_right = math.acos(target_angle_right_cos)
        print("Target Angle:", math.degrees(target_angle))
        print("Target Angle L:", math.degrees(target_angle_left))
        print("Target Angle R:", math.degrees(target_angle_right))
        self._zmq_pub.zmqPubDoubles("distangle", 0.0, distance, angle_h_robot, target_angle)

    def _target_calc_solvepnp(self, target):
        image_points = numpy.array([target.left_box[0].left,
                                    target.left_box[0].bottom,
                                    target.left_box[0].top,
                                    target.left_box[0].right,
                                    target.right_box[0].left,
                                    target.right_box[0].bottom,
                                    target.right_box[0].top,
                                    target.right_box[0].right])
        success, rotation_vector, translation_vector = \
            cv2.solvePnP(self.model_points, image_points, 
                        self._undistorter.camera_matrix, 
                        self._undistorter.dist_coeffs, 
                        flags=cv2.SOLVEPNP_ITERATIVE)
        if success:
            print(translation_vector)
        else:
            print("SolvePnP failed")

    def process(self, pipeline):
        boxes = [cv2.minAreaRect(contour) for contour in pipeline.convex_hulls_output]
        # minAreaRect returns ((x, y), (width, height), angle), angle is -90 to 0
        # see https://stackoverflow.com/questions/15956124/minarearect-angles-unsure-about-the-angle-returned
        boxes = [[cv2.boxPoints(box), box] for box in boxes] # Create (corner coords, arearect) tuples

        boxes.sort(key=lambda box: box[1][0][0]) # Sort by x position
        targets = []
        # Iterate over pairs of boxes (saving to VisionTargets) and find centers
        for first, second in zip(boxes[::2], boxes[1::2]):
            targets.append(self.VisionTarget(first, second, center = \
            self._find_center(first, second)))
        # Other set of combinations
        # Not all combinations will be valid, that is expected
        for first, second in zip(boxes[1::2], boxes[2::2]):
            targets.append(self.VisionTarget(first, second, center = \
            self._find_center(first, second)))
        # Sort coordinates within each box to [left, bottom, top, right] in a TapeBox
        for target in targets:
            new_box = [-1, -1, -1, -1]
            sorted_box = sorted(target.left_box[0], key=lambda point: point[0]) # X pos sort
            new_box[0] = sorted_box[0] # Left
            new_box[3] = sorted_box[3] # Right
            sorted_box = sorted(target.left_box[0], key=lambda point: point[1]) # Y pos sort
            new_box[1] = sorted_box[3] # Bottom
            new_box[2] = sorted_box[0] # Top
            target.left_box[0] = self.TapeBox(*new_box)
            new_box = [-1, -1, -1, -1]
            sorted_box = sorted(target.right_box[0], key=lambda point: point[0]) # X pos sort
            new_box[0] = sorted_box[0] # Left
            new_box[3] = sorted_box[3] # Right
            sorted_box = sorted(target.right_box[0], key=lambda point: point[1]) # Y pos sort
            new_box[1] = sorted_box[3] # Bottom
            new_box[2] = sorted_box[0] # Top
            target.right_box[0] = self.TapeBox(*new_box)
        # Filter out target possibilities without correct tilts
        targets = list(filter(lambda target: self._tilted_right(target.left_box[0]) and \
        self._tilted_left(target.right_box[0]), targets))
        # Find target with lowest y value (closest to camera)
        # targets.sort(key=lambda target: target.center[1], reverse=True)
        # Pick target closest to center of frame (x), may need to use angle_h if off center camera
        targets.sort(key = lambda target: abs(target.center[0]-self.half_width_pixels))
        print([abs(target.center[0]-self.half_width_pixels) for target in targets])
        try:
            target = targets[0]
        except IndexError:
            print("No valid targets found")
            return
        self._target_calc_solvepnp(target)


class ExtraProcessingHatch:
    # Camera constants
    horiz_FOV = 25.18 * 2
    vert_FOV = 52.696
    height = 31
    vert_angle = 52 # How far down the camera is pointed
    horiz_angle = 0 # How far to the right the camera is pointed
    horiz_offset = 0 # How far to the right the camera is shifted
    width_pixels = 160
    height_pixels = 120

    # Calculated constants
    half_height_pixels = height_pixels / 2
    half_width_pixels = width_pixels / 2
    horiz_tan = math.tan(math.radians(horiz_FOV/2))
    vert_tan = math.tan(math.radians(vert_FOV/2))

    def __init__(self, zmq_pub):
        self._zmq_pub = zmq_pub

    def process(self, pipeline):
        contours = pipeline.filter_contours_output
        contours.sort(key=cv2.contourArea) # Find largest area contour
        try:
            x, y, w, h = cv2.boundingRect(contours[0])
            center = ((x+(w/2)), (y+(h/2)));

            angle_h = (math.degrees(
                        math.atan(((center[0]-self.half_width_pixels)*self.horiz_tan
                        /self.half_width_pixels)))) - self.horiz_angle
            angle_v = (math.degrees(
                        math.atan(((center[1]-self.half_height_pixels)*-1*self.vert_tan
                        /self.half_height_pixels)))) - self.vert_angle
            distance = math.tan(math.radians(90-abs(angle_v))) * self.height
            if self.horiz_offset != 0:
                horiz_distance = math.tan(math.radians(angle_h)) * distance
                horiz_distance += self.horiz_offset
                angle_h = math.degrees(math.atan(horiz_distance/distance))
            self._zmq_pub.zmqPubDoubles("distangle", 0.0, distance, angle_h)
            print("Distance:", distance)
            print("Angle:", angle_h)
        except IndexError:
            # No contours found
            print("Ran pipeline but no contours")


def getTimeMS():
    return int(round(time.time() * 1000))

class ZmqPubIF:
    """
    A simple zmq wrapper class that provides publishing functionality
    """
    def __init__(self, port):
        self.socket = context.socket(zmq.PUB)
        self.socket.bind("tcp://*:%s" % port)

    def zmqPubStr(self, topic, dataStr):
        # The topic is the first string sent
        self.socket.send_string(topic, zmq.SNDMORE)
        # Here we're assuming we're just sending a string as payload
        self.socket.send_string(dataStr)

    def zmqPubDoubles(self, topic, *doubles):
        self.socket.send_string(topic, zmq.SNDMORE)
        for double in doubles[:-1]:
            self.socket.send(struct.pack("!d", double), zmq.SNDMORE)
        self.socket.send(struct.pack("!d", doubles[-1]))

class ZmqRecvIF:
    """
    A simple class to recieve commands over ZeroMQ
    """
    def __init__(self, port):
        self.socket = context.socket(zmq.PULL)
        self.socket.bind("tcp://*:%s" % port)
        self.flags = 0

    def recv_command(self):
        return self.socket.recv_multipart(self.flags)

    def set_blocking(self, block):
        self.flags = 0 if block else zmq.NOBLOCK


context = zmq.Context()
def main():
    zmq_publish_port = "5556"
    zmq_recv_port = "5555"
    Pipeline = namedtuple("Pipeline", ["GRIP_pipeline", "processing_class", \
    "camera", "undistorter"])

    print('Initializing ZMQ Publisher')
    zmq_pub = ZmqPubIF(zmq_publish_port)

    print('Initializing ZMQ Reciever')
    zmq_recv = ZmqRecvIF(zmq_recv_port)

    print('Creating pipelines')
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
    # cap.set(cv2.CAP_PROP_BRIGHTNESS, 0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)
    cap.set(cv2.CAP_PROP_FPS, 30)
    cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0.25) # Disable auto exposure
    cap.set(cv2.CAP_PROP_EXPOSURE, 0.002) # Minimum exposure, should be 0 on LifeCam/XBox and 0.003 on new ELP
    cap.set(cv2.CAP_PROP_SHARPNESS, 0) # Removes haloing on ELP, not needed on LifeCam or XBox
    delivery_undistorter = DeliveryUndistorter()
    pipelines = {b"none" : None, \
    b"delivery" : Pipeline(GripPipelineRetro(), \
    ExtraProcessingDelivery(zmq_pub, delivery_undistorter), cap, delivery_undistorter),
    b"hatch" : Pipeline(GripPipelineHatch(), \
    ExtraProcessingHatch(zmq_pub), cap, None)}
    pipeline = None

    print('Running pipeline')
    while cap.isOpened():
        try:
            zmq_command = zmq_recv.recv_command()
            print("Recieved command", zmq_command[0])
            if zmq_command[0] == b"set_pipeline":
                print("Set pipeline to {}".format(zmq_command[1].decode()))
                pipeline = pipelines[zmq_command[1]]
                if pipeline is not None:
                    zmq_recv.set_blocking(False)
                    pipeline.camera.grab() # Flush the 1 frame that could be in the buffer
                else:
                    zmq_recv.set_blocking(True) # Block while waiting for commands when not running a pipeline
            elif zmq_command[0] == b"write_frame":
                if pipeline is not None:
                    cv2.imwrite("capture.jpg", pipeline.camera.read()[1])
                    print("Saved frame to capture.jpg")
                else:
                    cv2.imwrite("capture.jpg", cap.read()[1])
                    print("Saved frame to capture.jpg using default camera")
        except zmq.ZMQError:
            # No command
            pass
        if pipeline is not None:
            have_frame, frame = pipeline.camera.read()
            try:
                # frame = pipeline.undistorter.undistort(frame)
                pass
            except AttributeError:
                # There is no undistorter
                pass
            if have_frame:
                pipeline.GRIP_pipeline.process(frame)
                pipeline.processing_class.process(pipeline.GRIP_pipeline)

    print('Capture closed')


if __name__ == '__main__':
    main()
